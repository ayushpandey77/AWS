{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "608e6899",
      "metadata": {
        "id": "608e6899"
      },
      "source": [
        "# AWS Assignment – Colab Workbook\n",
        "Organized answers with ready-to-run commands and code snippets. Replace placeholder values (**ALL_CAPS**) with your own (e.g., bucket names, regions, ARNs).\n",
        "\n",
        "> Tip: In Colab, you can execute shell commands by prefixing with `!` and install packages with `pip`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c777d27",
      "metadata": {
        "id": "4c777d27"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "\n",
        "## Setup: Variables & AWS CLI\n",
        "\n",
        "Set your variables here. In Colab, you can authenticate to AWS by configuring credentials (`aws configure`) or using environment variables/roles (when running on an EC2/SageMaker/Cloud9 instance)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8026975a",
      "metadata": {
        "id": "8026975a"
      },
      "source": [
        "<a id=\"q1\"></a>\n",
        "\n",
        "## Q1: Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for data analysis and latency-sensitive applications?\n",
        "\n",
        "- **Region**: A separate geographic area (e.g., `ap-south-1` in Mumbai) consisting of multiple isolated locations (AZs). Data residency, regulatory control, and service availability vary by region.\n",
        "- **Availability Zone (AZ)**: One or more discrete data centers within a region with independent power/network. Deploying across multiple AZs improves **high availability** and **fault tolerance**.\n",
        "- **Edge Location**: Part of the **Amazon CloudFront** and **Global Accelerator** network, placed near end users to cache content or accelerate traffic, reducing **latency**.\n",
        "\n",
        "**Why it matters**:\n",
        "- Analytics jobs near data sources (same **Region**) avoid cross-region data transfer cost/latency.\n",
        "- Distributing compute across **AZs** reduces downtime for pipelines and streaming consumers.\n",
        "- **Edge** caching/ingestion lowers latency for dashboards and streaming ingestion from global producers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29164fee",
      "metadata": {
        "id": "29164fee"
      },
      "source": [
        "<a id=\"q2\"></a>\n",
        "\n",
        "## Q2: Using the AWS CLI, list all available AWS regions. Share the command and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1adbeedb",
      "metadata": {
        "id": "1adbeedb"
      },
      "outputs": [],
      "source": [
        "# If AWS CLI is installed & configured, run:\n",
        "# !aws ec2 describe-regions --all-regions --query 'Regions[].{Region:RegionName, OptInStatus:OptInStatus}' --output table\n",
        "\n",
        "# Alternate (names only):\n",
        "# !aws ec2 describe-regions --all-regions --query 'Regions[].RegionName' --output text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65dd8204",
      "metadata": {
        "id": "65dd8204"
      },
      "source": [
        "<a id=\"q3\"></a>\n",
        "\n",
        "## Q3: Create a new IAM user with least privilege access to Amazon S3. Share attached policies.\n",
        "\n",
        "**Principles**: grant only the S3 permissions required (e.g., read/write to a specific bucket path). Avoid `*` where possible.\n",
        "\n",
        "**Steps (CLI)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98d4814",
      "metadata": {
        "id": "e98d4814"
      },
      "outputs": [],
      "source": [
        "# Create user (programmatic access via access key)\n",
        "# !aws iam create-user --user-name s3-data-user\n",
        "\n",
        "# Attach an inline policy allowing least-privilege S3 access to a single bucket/prefix\n",
        "import json\n",
        "least_priv_policy = {\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Sid\": \"ListBucketAtPrefix\",\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\"s3:ListBucket\"],\n",
        "      \"Resource\": [f\"arn:aws:s3:::{BUCKET}\"],\n",
        "      \"Condition\": {\"StringLike\": {\"s3:prefix\": [f\"{DATA_PREFIX}/*\"]}}\n",
        "    },\n",
        "    {\n",
        "      \"Sid\": \"RWObjectsUnderPrefix\",\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\"s3:GetObject\",\"s3:PutObject\",\"s3:DeleteObject\",\"s3:AbortMultipartUpload\"],\n",
        "      \"Resource\": [f\"arn:aws:s3:::{BUCKET}/{DATA_PREFIX}/*\"]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "print(json.dumps(least_priv_policy, indent=2))\n",
        "\n",
        "# Put the inline policy (save first):\n",
        "# with open(\"least_priv_policy.json\",\"w\") as f: json.dump(least_priv_policy, f)\n",
        "# !aws iam put-user-policy --user-name s3-data-user --policy-name S3PrefixLeastPrivilege --policy-document file://least_priv_policy.json\n",
        "\n",
        "# (Optional) Create access keys for the user (handle securely, never commit to source control!)\n",
        "# !aws iam create-access-key --user-name s3-data-user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a824285",
      "metadata": {
        "id": "6a824285"
      },
      "source": [
        "<a id=\"q4\"></a>\n",
        "\n",
        "## Q4: Compare S3 storage classes (Standard, Intelligent-Tiering, Glacier). When to use each in analytics?\n",
        "\n",
        "| Storage Class | Durability/Availability | Cost | Use in Analytics |\n",
        "|---|---|---|---|\n",
        "| **Standard** | 11x9s / high | Higher | Active datasets, staging/landing zones, frequent reads/writes |\n",
        "| **Intelligent‑Tiering** | 11x9s / high | Optimizes | Unpredictable access; auto‑moves between frequent/infrequent tiers without operational work |\n",
        "| **Glacier (Flexible/Deep Archive)** | 11x9s / variable retrieval times | Lowest at rest | Long‑term retention, rarely accessed raw data, audit archives; retrieve before batch analysis |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd454f5",
      "metadata": {
        "id": "0fd454f5"
      },
      "source": [
        "<a id=\"q5\"></a>\n",
        "\n",
        "## Q5: Create an S3 bucket and upload a sample dataset; enable versioning and show versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ca163c",
      "metadata": {
        "id": "11ca163c"
      },
      "outputs": [],
      "source": [
        "# Create bucket (name must be globally unique)\n",
        "# !aws s3 mb s3://$BUCKET --region $REGION\n",
        "\n",
        "# Enable versioning\n",
        "# !aws s3api put-bucket-versioning --bucket $BUCKET --versioning-configuration Status=Enabled\n",
        "\n",
        "# Upload sample dataset\n",
        "sample_csv = \"name,age\\nAsha,29\\nRavi,33\\nZoya,41\\n\"\n",
        "with open(TEST_FILE, \"w\") as f: f.write(sample_csv)\n",
        "print(\"Wrote local sample CSV:\", TEST_FILE)\n",
        "\n",
        "# First version\n",
        "# !aws s3 cp $TEST_FILE s3://$BUCKET/$DATA_PREFIX/$TEST_FILE\n",
        "\n",
        "# Modify & upload again to create a second version\n",
        "sample_csv_v2 = \"name,age\\nAsha,30\\nRavi,33\\nZoya,41\\n\"\n",
        "with open(TEST_FILE, \"w\") as f: f.write(sample_csv_v2)\n",
        "print(\"Updated CSV to create v2.\")\n",
        "\n",
        "# Second version\n",
        "# !aws s3 cp $TEST_FILE s3://$BUCKET/$DATA_PREFIX/$TEST_FILE\n",
        "\n",
        "# List object versions\n",
        "# !aws s3api list-object-versions --bucket $BUCKET --prefix $DATA_PREFIX/$TEST_FILE --query 'Versions[].{Id:VersionId,LastModified:LastModified,IsLatest:IsLatest}' --output table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa027d9",
      "metadata": {
        "id": "2fa027d9"
      },
      "source": [
        "<a id=\"q6\"></a>\n",
        "\n",
        "## Q6: Lifecycle policy (move to Glacier after 30 days, delete after 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6260e1",
      "metadata": {
        "id": "9b6260e1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "lifecycle_policy = {\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"ToGlacierThenExpire\",\n",
        "      \"Filter\": {\"Prefix\": f\"{DATA_PREFIX}/\"},\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Transitions\": [\n",
        "        {\"Days\": 30, \"StorageClass\": \"GLACIER\"}\n",
        "      ],\n",
        "      \"Expiration\": {\"Days\": 90}\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "print(json.dumps(lifecycle_policy, indent=2))\n",
        "\n",
        "# with open(\"lifecycle.json\",\"w\") as f: json.dump(lifecycle_policy, f)\n",
        "# !aws s3api put-bucket-lifecycle-configuration --bucket $BUCKET --lifecycle-configuration file://lifecycle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602fd1b2",
      "metadata": {
        "id": "602fd1b2"
      },
      "source": [
        "<a id=\"q7\"></a>\n",
        "\n",
        "## Q7: Compare RDS, DynamoDB, and Redshift with one data-pipeline use case each\n",
        "\n",
        "- **Amazon RDS (relational)**: OLTP, transactional integrity (ACID), SQL joins. *Use case*: store cleaned, modeled app data used by downstream analytics and operational dashboards.\n",
        "- **Amazon DynamoDB (NoSQL key‑value/Document)**: massive scale, single‑digit ms latency, serverless ops. *Use case*: capture clickstream/session events with partition keys for hot ingestion; later ETL to S3.\n",
        "- **Amazon Redshift (MPP analytical columnar)**: petabyte‑scale, columnar storage, parallel queries. *Use case*: star‑schema warehouse powering BI with complex aggregations and joins.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f3a154b",
      "metadata": {
        "id": "9f3a154b"
      },
      "source": [
        "<a id=\"q8\"></a>\n",
        "\n",
        "## Q8: Create a DynamoDB table and insert 3 records manually. Lambda adds records on S3 uploads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a51a33",
      "metadata": {
        "id": "70a51a33"
      },
      "outputs": [],
      "source": [
        "# Create table (Partition key: pk, Sort key: sk)\n",
        "# !aws dynamodb create-table --table-name $DDB_TABLE \\\n",
        "#   --attribute-definitions AttributeName=pk,AttributeType=S AttributeName=sk,AttributeType=S \\\n",
        "#   --key-schema AttributeName=pk,KeyType=HASH AttributeName=sk,KeyType=RANGE \\\n",
        "#   --billing-mode PAY_PER_REQUEST\n",
        "\n",
        "# Insert 3 records\n",
        "# !aws dynamodb put-item --table-name $DDB_TABLE --item '{\"pk\":{\"S\":\"user#1\"},\"sk\":{\"S\":\"evt#001\"},\"info\":{\"S\":\"seed\"}}'\n",
        "# !aws dynamodb put-item --table-name $DDB_TABLE --item '{\"pk\":{\"S\":\"user#2\"},\"sk\":{\"S\":\"evt#001\"},\"info\":{\"S\":\"seed\"}}'\n",
        "# !aws dynamodb put-item --table-name $DDB_TABLE --item '{\"pk\":{\"S\":\"user#3\"},\"sk\":{\"S\":\"evt#001\"},\"info\":{\"S\":\"seed\"}}'\n",
        "\n",
        "lambda_code = (\n",
        "\"import json, boto3, os, urllib.parse\\n\"\n",
        "\"dynamo = boto3.resource(\\\"dynamodb\\\")\\n\"\n",
        "\"table = dynamo.Table(os.environ[\\\"TABLE_NAME\\\"])\\n\\n\"\n",
        "\"def handler(event, context):\\n\"\n",
        "\"    for record in event.get(\\\"Records\\\", []):\\n\"\n",
        "\"        s3 = record.get(\\\"s3\\\", {})\\n\"\n",
        "\"        bucket = s3.get(\\\"bucket\\\", {}).get(\\\"name\\\")\\n\"\n",
        "\"        key = urllib.parse.unquote_plus(s3.get(\\\"object\\\", {}).get(\\\"key\\\", \\\"\\\"))\\n\"\n",
        "\"        size = s3.get(\\\"object\\\", {}).get(\\\"size\\\", 0)\\n\"\n",
        "\"        table.put_item(Item={\\n\"\n",
        "\"            'pk': f's3#{bucket}',\\n\"\n",
        "\"            'sk': f'object#{key}',\\n\"\n",
        "\"            'size': int(size),\\n\"\n",
        "\"            'ts': record.get('eventTime')\\n\"\n",
        "\"        })\\n\"\n",
        "\"    return {'status': 'ok'}\\n\"\n",
        ")\n",
        "print(lambda_code)\n",
        "\n",
        "# Create the Lambda, add S3 trigger, and environment variable TABLE_NAME=$DDB_TABLE.\n",
        "# Grant IAM permissions: dynamodb:PutItem on the table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba8aaf1",
      "metadata": {
        "id": "6ba8aaf1"
      },
      "source": [
        "<a id=\"q9\"></a>\n",
        "\n",
        "## Q9: What is serverless computing? Pros & cons of Lambda for data pipelines\n",
        "\n",
        "**Serverless** means you focus on code; the provider manages servers, scaling, and patching. You pay per use.\n",
        "\n",
        "**Pros**:\n",
        "- Auto‑scaling and high availability by default\n",
        "- Pay‑per‑invocation; cost‑efficient for spiky workloads\n",
        "- Easy event integrations (S3, SQS, EventBridge, Kinesis)\n",
        "\n",
        "**Cons**:\n",
        "- Cold starts (latency) for some runtimes/VPC configs\n",
        "- Execution time/memory/ephemeral storage limits\n",
        "- Stateful/long‑running tasks need orchestration (e.g., Step Functions, Glue, ECS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74179b72",
      "metadata": {
        "id": "74179b72"
      },
      "source": [
        "<a id=\"q10\"></a>\n",
        "\n",
        "## Q10: Lambda function triggered by S3 that logs file name, size, timestamp to CloudWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4709ae27",
      "metadata": {
        "id": "4709ae27"
      },
      "outputs": [],
      "source": [
        "lambda_logger = (\n",
        "\"import json, logging, urllib.parse\\n\\n\"\n",
        "\"logger = logging.getLogger()\\n\"\n",
        "\"logger.setLevel('INFO')\\n\\n\"\n",
        "\"def handler(event, context):\\n\"\n",
        "\"    for record in event.get('Records', []):\\n\"\n",
        "\"        s3 = record.get('s3', {})\\n\"\n",
        "\"        bucket = s3.get('bucket', {}).get('name')\\n\"\n",
        "\"        key = urllib.parse.unquote_plus(s3.get('object', {}).get('key', ''))\\n\"\n",
        "\"        size = s3.get('object', {}).get('size', 0)\\n\"\n",
        "\"        ts = record.get('eventTime')\\n\"\n",
        "\"        logger.info({'bucket': bucket, 'key': key, 'size': size, 'timestamp': ts})\\n\"\n",
        "\"    return {'status':'ok'}\\n\"\n",
        ")\n",
        "print(lambda_logger)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2d932af",
      "metadata": {
        "id": "e2d932af"
      },
      "source": [
        "<a id=\"q11\"></a>\n",
        "\n",
        "## Q11: Use AWS Glue to crawl S3 data, create a Data Catalog table, and run a job to convert CSV → Parquet\n",
        "**Steps**:\n",
        "1. Create a **Crawler** pointing to `s3://BUCKET/datasets/sample/` with an IAM role that can read S3 and write to the **Glue Data Catalog** (database like `analytics_db`).\n",
        "2. Run the crawler to create/update a table (e.g., `analytics_db.people`).\n",
        "3. Create a **Glue Spark** job with the script below to read CSV and write Parquet to `s3://BUCKET/output/people_parquet/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "953fb33c",
      "metadata": {
        "id": "953fb33c"
      },
      "outputs": [],
      "source": [
        "glue_job_script = (\n",
        "\"import sys\\n\"\n",
        "\"from awsglue.transforms import *\\n\"\n",
        "\"from awsglue.utils import getResolvedOptions\\n\"\n",
        "\"from pyspark.context import SparkContext\\n\"\n",
        "\"from awsglue.context import GlueContext\\n\"\n",
        "\"from awsglue.job import Job\\n\\n\"\n",
        "\"args = getResolvedOptions(sys.argv, ['JOB_NAME','SOURCE_S3','OUTPUT_S3'])\\n\"\n",
        "\"sc = SparkContext()\\n\"\n",
        "\"glueContext = GlueContext(sc)\\n\"\n",
        "\"spark = glueContext.spark_session\\n\"\n",
        "\"job = Job(glueContext); job.init(args['JOB_NAME'], args)\\n\\n\"\n",
        "\"df = spark.read.option('header','true').csv(args['SOURCE_S3'])\\n\"\n",
        "\"df.write.mode('overwrite').parquet(args['OUTPUT_S3'])\\n\\n\"\n",
        "\"job.commit()\\n\"\n",
        ")\n",
        "print(glue_job_script)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2c1582",
      "metadata": {
        "id": "cf2c1582"
      },
      "source": [
        "<a id=\"q12\"></a>\n",
        "\n",
        "## Q12: Difference between Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics + examples\n",
        "\n",
        "- **Kinesis Data Streams (KDS)**: real‑time, low‑latency streaming service with shards and consumer apps. *Example*: custom consumers (Lambda/Flink) process clickstreams within seconds.\n",
        "- **Kinesis Data Firehose (KDF)**: fully managed delivery from sources to destinations (S3, Redshift, OpenSearch) with minimal config; buffering/format conversion built‑in. *Example*: deliver IoT events to S3 in Parquet every 1–5 minutes.\n",
        "- **Kinesis Data Analytics (KDA)**: run SQL or Apache Flink apps on streaming data. *Example*: continuous SQL aggregations on KDS, pushing results to Firehose or Lambda.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2903db77",
      "metadata": {
        "id": "2903db77"
      },
      "source": [
        "<a id=\"q13\"></a>\n",
        "\n",
        "## Q13: What is columnar storage and how does it benefit Redshift performance?\n",
        "\n",
        "Columnar storage saves values column‑by‑column instead of row‑by‑row. Benefits for analytics:\n",
        "- **I/O reduction**: scan only needed columns for SELECTs.\n",
        "- **Compression**: similar adjacent values compress well, reducing disk/memory usage.\n",
        "- **Vectorized processing**: efficient CPU utilization for aggregates/scans.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0810ef53",
      "metadata": {
        "id": "0810ef53"
      },
      "source": [
        "<a id=\"q14\"></a>\n",
        "\n",
        "## Q14: Load a CSV from S3 into Redshift using COPY (schema, command, sample query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8bef57d",
      "metadata": {
        "id": "d8bef57d"
      },
      "outputs": [],
      "source": [
        "redshift_schema_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS public.people (\n",
        "  name VARCHAR(50),\n",
        "  age  INTEGER\n",
        ");\n",
        "\"\"\"\n",
        "copy_cmd = \"\"\"\n",
        "COPY public.people\n",
        "FROM '{S3_SOURCE}'\n",
        "IAM_ROLE '{REDSHIFT_IAM_ROLE_ARN}'\n",
        "CSV IGNOREHEADER 1 REGION '{REGION}';\n",
        "\"\"\"\n",
        "sample_query = \"SELECT COUNT(*) AS rows_loaded, AVG(age)::numeric(10,2) AS avg_age FROM public.people;\"\n",
        "print(\"-- Schema DDL --\\n\", redshift_schema_sql)\n",
        "print(\"\\n-- COPY command (replace placeholders in braces) --\\n\", copy_cmd)\n",
        "print(\"\\n-- Sample query --\\n\", sample_query)\n",
        "# Replace {S3_SOURCE}, {REDSHIFT_IAM_ROLE_ARN}, and {REGION} with your values before executing in Redshift.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badae790",
      "metadata": {
        "id": "badae790"
      },
      "source": [
        "<a id=\"q15\"></a>\n",
        "\n",
        "## Q15: Role of the Glue Data Catalog in Athena; how schema‑on‑read works\n",
        "\n",
        "- **Glue Data Catalog** stores table/partition metadata (locations, schemas, SerDes) for data in S3.\n",
        "- **Athena** uses this metadata to interpret files at query time (**schema‑on‑read**) rather than enforcing schema during ingestion. You can change the schema independently, and the same data can be queried by different tools without rewriting files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee6c248",
      "metadata": {
        "id": "cee6c248"
      },
      "source": [
        "<a id=\"q16\"></a>\n",
        "\n",
        "## Q16: Create an Athena table from S3 data using Glue Catalog; run a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abbcdf64",
      "metadata": {
        "id": "abbcdf64"
      },
      "outputs": [],
      "source": [
        "athena_ddl = \"\"\"\n",
        "CREATE DATABASE IF NOT EXISTS analytics_db;\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS analytics_db.people (\n",
        "  name string,\n",
        "  age  int\n",
        ")\n",
        "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES ('separatorChar'=',','quoteChar'='\\\"','escapeChar'='\\\\')\n",
        "LOCATION '{S3_SOURCE}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\"\n",
        "athena_query = \"SELECT name, age FROM analytics_db.people WHERE age >= 30 ORDER BY age DESC;\"\n",
        "print(\"-- Athena DDL (replace {S3_SOURCE}) --\\n\", athena_ddl)\n",
        "print(\"\\n-- Sample Query --\\n\", athena_query)\n",
        "# In Athena console, set a Query Result Location in S3 before running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a985d54b",
      "metadata": {
        "id": "a985d54b"
      },
      "source": [
        "<a id=\"q17\"></a>\n",
        "\n",
        "## Q17: How Amazon QuickSight supports BI in a serverless data architecture; SPICE & embedded dashboards\n",
        "\n",
        "- **Serverless BI**: QuickSight connects to Athena/Redshift/S3; no servers to manage for the BI layer.\n",
        "- **SPICE**: QuickSight's in‑memory engine for fast, interactive analysis; you can import data into SPICE to accelerate dashboards and control refresh schedules.\n",
        "- **Embedded dashboards**: securely embed dashboards into apps/portals with row‑level security and AWS IAM/Federation integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5856a4b",
      "metadata": {
        "id": "d5856a4b"
      },
      "source": [
        "<a id=\"q18\"></a>\n",
        "\n",
        "## Q18: Connect QuickSight to Athena or Redshift; build a dashboard (calc field + filter)\n",
        "\n",
        "**Steps**:\n",
        "1. In QuickSight, create a **Data Source** to **Athena** (or **Redshift**). Grant QuickSight the required IAM permissions.\n",
        "2. Choose your database/table (e.g., `analytics_db.people`) and **import to SPICE** for speed.\n",
        "3. Create a dataset; add a **calculated field** (e.g., `is_senior = ifelse({age} >= 60, 1, 0)`).\n",
        "4. Build visuals (e.g., table + KPI); add a **Filter** on `age` or a date field.\n",
        "5. Publish the analysis as a **dashboard**. (Optional) Use the **embedding** options for apps/portals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e00682f",
      "metadata": {
        "id": "4e00682f"
      },
      "source": [
        "<a id=\"q19\"></a>\n",
        "\n",
        "## Q19: CloudWatch vs CloudTrail in a data analytics pipeline\n",
        "\n",
        "- **CloudWatch**: metrics, logs, alarms, dashboards. Use for pipeline health (Lambda duration/errors, Glue job logs, Kinesis throughput), alerting on failures or lag.\n",
        "- **CloudTrail**: governance/auditing of **API activity** across AWS accounts. Use to trace who changed an S3 bucket policy or who ran a Glue job, and to investigate incidents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eed2c46",
      "metadata": {
        "id": "1eed2c46"
      },
      "source": [
        "<a id=\"q20\"></a>\n",
        "\n",
        "## Q20: End‑to‑end data analytics pipeline (example)\n",
        "\n",
        "**Ingestion**: App/IoT → **Kinesis Firehose** → **S3 landing**  \n",
        "**Processing**: S3 event → **Lambda** for light validation; batch ETL in **Glue** (CSV→Parquet, partitioned by date) → **S3 curated**  \n",
        "**Catalog/Query**: **Glue Data Catalog** tables over curated S3; **Athena** for ad‑hoc SQL; **Redshift** for complex joins/BI marts  \n",
        "**Visualization**: **QuickSight** (SPICE for speed, embedded dashboard)  \n",
        "**Ops**: **CloudWatch** for logs/alarms/dashboards; **CloudTrail** for audit; **IAM** least privilege\n",
        "\n",
        "**Why these choices**: serverless, scalable, cost‑efficient; Parquet + partitioning minimizes scan costs; separation of landing/curated zones improves governance; Glue/Athena/Redshift cover ELT and analytics at multiple scales.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}